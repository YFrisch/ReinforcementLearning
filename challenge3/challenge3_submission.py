"""
Submission template for Programming Challenge 3: Policy Gradient Methods.


Fill in submission info and implement 6 functions:

- load_reinforce_policy
- train_reinforce_policy
- load_npg_policy
- train_npg_policy
- load_nes_policy
- train_nes_policy

Keep Monitor files generated by Gym while learning within your submission.
Example project structure:

challenge3_submission/
  - challenge3_submission.py
  - reinforce.py
  - npg.py
  - nes.py
  - reinforce_eval/
  - npg_eval/
  - nes_eval/
  - reinforce_train/
  - npg_train/
  - nes_train/
  - supplementary/

Directories `npg_eval/`, `npg_train/`, etc. are autogenerated by Gym (see below).
Put all additional results into the `supplementary` directory.

Performance of the policies returned by `load_xxx_policy` functions
will be evaluated and used to determine the winner of the challenge.
Learning progress and learning algorithms will be checked to confirm
correctness and fairness of implementation. Supplementary material
will be manually analyzed to identify outstanding submissions.
"""

import numpy as np

info = dict(
    group_number=None,  # change if you are an existing seminar/project group
    authors="John Doe; Lorem Ipsum; Foo Bar",
    description="Explain what your code does and how. "
                "Keep this description short "
                "as it is not meant to be a replacement for docstrings "
                "but rather a quick summary to help the grader.")


def load_reinforce_policy():
    """
    Load pretrained REINFORCE policy from file.

    The policy must return a continuous action `a`
    that can be directly passed to `Levitation-v1` env.

    :return: function pi: s -> a
    """
    return lambda obs: np.array([3.1415])


def train_reinforce_policy(env):
    """
    Execute your implementation of the REINFORCE learning algorithm.

    This function should start your code placed in a separate file.

    :param env: gym.Env
    :return: function pi: s -> a
    """
    return lambda obs: np.array([2.7183])


def load_npg_policy():
    """
    Load pretrained NPG policy from file.

    The policy must return a continuous action `a`
    that can be directly passed to `BallBalancerSim-v0` env.

    :return: function pi: s -> a
    """
    return lambda obs: np.array([1.6180])


def train_npg_policy(env):
    """
    Execute your implementation of the NPG learning algorithm.

    This function should start your code placed in a separate file.

    :param env: gym.Env
    :return: function pi: s -> a
    """
    return lambda obs: np.array([0.5772])


def load_nes_policy():
    """
    Load pretrained NES policy from file.

    The policy must return a continuous action `a`
    that can be directly passed to `BallBalancerSim-v0` env.

    :return: function pi: s -> a
    """
    return lambda obs: np.array([299792458.0])


def train_nes_policy(env):
    """
    Execute your implementation of the NES learning algorithm.

    This function should start your code placed in a separate file.

    :param env: gym.Env
    :return: function pi: s -> a
    """
    return lambda obs: np.array([9.10938356 * 1e-31])



# ==== Example evaluation
def main():
    import gym
    from gym.wrappers.monitor import Monitor
    import quanser_robots

    def evaluate(env, policy, num_evlas=25):
        ep_returns = []
        for eval_num in range(num_evlas):
            episode_return = 0
            dones = False
            obs = env.reset()
            while not dones:
                action = policy(obs)
                obs, rewards, dones, info = env.step(action)
                episode_return += rewards
            ep_returns.append(episode_return)
        return ep_returns

    def render(env, policy):
        obs = env.reset()
        done = False
        while not done:
            env.render()
            act = policy(obs)
            obs, _, done, _ = env.step(act)

    def check(env, policy):
        render(env, policy)
        ret_all = evaluate(env, policy)
        print(np.mean(ret_all), np.std(ret_all))
        env.close()

    # REINFORCE I: Check learned policy
    env = Monitor(gym.make('Levitation-v1'), 'reinforce_eval')
    policy = load_reinforce_policy()
    check(env, policy)

    # REINFORCE II: Check learning procedure
    env = Monitor(gym.make('Levitation-v1'), 'reinforce_train', video_callable=False)
    policy = train_reinforce_policy(env)
    check(env, policy)

    # NPG I: Check learned policy
    env = Monitor(gym.make('BallBalancerSim-v0'), 'npg_eval')
    policy = load_npg_policy()
    check(env, policy)

    # NPG II: Check learning procedure
    env = Monitor(gym.make('BallBalancerSim-v0'), 'npg_train', video_callable=False)
    policy = train_npg_policy(env)
    check(env, policy)

    # NES I: Check learned policy
    env = Monitor(gym.make('BallBalancerSim-v0'), 'nes_eval')
    policy = load_nes_policy()
    check(env, policy)

    # NES II: Check learning procedure
    env = Monitor(gym.make('BallBalancerSim-v0'), 'nes_train', video_callable=False)
    policy = train_nes_policy(env)
    check(env, policy)


if __name__ == '__main__':
    main()
